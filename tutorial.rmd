---
title: "3 Steps To Become A Data Scientist"
author: "Priyanka Kishore & Alisha Varma"
date: "5/17/2020"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
So you want to be a Data Scientist and don’t know where to start? Well, you’ve come to the right place. 

Today you’ll learn a little about the economy, a whole bunch of data science principles and practice and where to stay on your next trip to New York City!


## Data Science Pipeline
First, let’s look at the data science pipeline. 

The data science pipeline starts with defining what major questions one wants to answer and subsequently **_acquiring and importing the relevant data_** to be analyzed.

Then, the data is viewed and **_data tidying_** must occur; where a rectangular data structure model is assumed and three requirements must be met. Each observation (called an entity) forms a row, each variable (called an attribute) forms a column and each observational unit (type of entity) forms a table.

Leading to the exploratory data analysis process, where the data is transformed and visualized. **_Data cleaning_** may be necessary for missing data. When handling missing data, the missing data may be removed, encoded or imputation (replace missing values with the mean of non-missing values) of a numeric variable may be necessary.

Hypothesis testing and machine learning (ML) modeling are the final steps before the data and its results can be communicated.

![Image of Pipeline](https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png)

_Source:_ https://r4ds.had.co.nz/explore-intro.html

## Economy & Vacations
The rise of companies like Airbnb has given rise to The Sharing Economy. The sharing economy is a model defined as the facilitation of goods and services on a peer-to-peer level usually through online community platforms. This new model has made it possible for a great deal of people to gain another source of income and for you to have an affordable vacation.

As more sharing economy companies have opened, like Airbinb and Uber, the way we vacation has changed. This change has been documented and open data on it is available.


## DataSet Used
The data we will be using in this tutorial is [_New York City Airbnb Open Data_](http://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data/data) from Kaggle. We will use this data to look at the relationships between types of housing and location.


# Preparing Data

Download the [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv).

In this section, we will learn how to load in our dataset, view the data in our dataset, and clean it up so it's easy for us to work with.

First, let's load in the following libraries so we can use certain functions:
```{r, message=FALSE}
# for data wranging
library(tidyverse)
library(dplyr)

# for data analysis
library(geosphere)
library(ggplot2)
library(broom)
```

## Loading Data

CSV files are files that include data which are "comma-separated values", meaning that data values are literally separated by commas.

After we've downloaded our CSV file from Kaggle into our working directory, we can use the `read_csv` function to load the CSV file data into our program's data frame, which is a table of the data.

```{r, message=FALSE}
# create a dataframe from our CSV file
airbnb_tab <- read.csv("AB_NYC_2019.csv", header=TRUE)
```

There are some attributes that we don't need for our purposes, like `host_id`, `host_name`, `minimum_nights`, `number_of_reviews`, `last_review`, `reviews_per_month`, and `calculated_host_listings_count`. So, let's remove these from our data frame:

```{r}
# a vector called "to_remove" that has the names of the attributes we don't want
to_remove <- c('host_id', 
              'host_name', 
               'minimum_nights', 
               'number_of_reviews', 
               'last_review', 
               'reviews_per_month', 
               'calculated_host_listings_count')

# removing attributes from data frame using "to_remove"
airbnb_tab = airbnb_tab[ , !(names(airbnb_tab) %in% to_remove)]
```

## Viewing Data

Here, we see the first 10 rows in our dataset:

```{r}
knitr::kable(head(airbnb_tab, n=10))
```

Some Notes:

* knitr::kable() is used to make the table "pretty" and easier to read

* head(df, n=10) is used to view the dataframe with a specific number of rows (head() is not always necessary, you can just list the data frame for it to render)

* df is where the dataframe goes, in this case airbnb_tab

* n = determines the number of rows visible, in this case 10
The following is a list of descriptions for the attributes of our data set:

Attribute | Description/Unit
------------ | -------------
`id` | Unique ID for each Airbnb listing
`name` | Name or description of the Airbnb listing
`neighbourhood_group` | Boroughs of New York (Manhattan, Brooklyn, Queens, Bronx, Staten Island)
`neighbourhood` | Neighborhoods of New York
`latitude` | Degrees of latitude, measures distance North and South from Equator
`longitude` | Degrees of longitude, measure distance East and West of Prime Meridian
`room_type` | Type of space offered (Entire home/apt, Private room, Shared room)
`price` | Price of listing, in US Dollars
`availability_365` | Number of days in a year when the listing is available for booking


## Tidying Data

Tidying Data entails the elements listed in the list below.

Elements of a tidy dataset:
1. Each observation/entity forms a row
1. Each variable/attribute forms a column
1. Each observational unit (type of entity) forms a column (i.e. not dependent on one another)

Our dataset is already tidy and meets the criteria above. Each entity is a row and each attribute is a column, where no entity is dependent on another.

However, if your data set is untidy, below is an example on a different small dataset, to show you what to do.

### Sample Tidying



# Exploratory Data Analysis

In this section, we begin exploring what our data can tell us using *visualizations*. This will help us to better understand our data and help us make decisions about how we may want to further manipulate the data to see something specific, or decide which methods are best for modelling and Machine Learning!

The main reason for exploratory data analysis, or EDA, is to help us find any problems in our data preparation and gain a sense of variable properties, such as central trends (mean), spread (variance), skew, outliers, and relationships between pairs of variables, like their correlation or covariance.

You can read more about EDA at [CMSC 320 EDA Lecture Notes](https://www.hcbravo.org/IntroDataSci/bookdown-notes/exploratory-data-analysis-visualization.html) by Professor Hector Corrado Bravo.

## Handling Missing Data

Recall that the attribute `availability_365` tells us how many days in the year that this particular listing is available for people to book. 

Notice that 0 is a value for some of the entities (Airbnb listings). It doesn't make much sense for us to look at entities that aren't available at all during the year. In fact, more than 17000 entities are listed at being available for 0 days out of the year! That's about 1/3 of our dataset.

We'll call this "missing data", and remove these entities from our dataset:

```{r}

airbnb_tab <- airbnb_tab %>%
  filter(availability_365 > 0) # filter() is used to filter the dataframe via specific conditions

knitr::kable(head(airbnb_tab, n=10))

```
Note that a way to handle missing data, as mentioned in the data science pipline section (data cleaning), is removing missing data togehter. Having 0 as a value for `availability_365` is form of missing data. 


## Visualizations


### Interactive Map

```{r}
library(leaflet)

# Creating NYC Map
nyc_map <- leaflet(airbnb_tab) %>%
  addTiles() %>%
  setView(
    lat=40.730610, 
    lng=-73.935242, 
    zoom=11)

nyc_map
```

```{r}
leaflet(airbnb_tab) %>% 
  addTiles() %>%
    addAwesomeMarkers(
      lng = ~longitude, 
      lat = ~latitude,
      icon = awesomeIcons(
              icon = 'ios-close',
              iconColor = 'black',
              library = 'ion',
              markerColor = ~ifelse(room_type == 'Entire home/apt', "green", 
                                    ifelse(room_type =='Private room', "orange", 
                                           "red"
                                    )
                            )
            ),
      
      ## Price Label
      label=~as.character(price),
    
      ## Clustering for identifying arrest density
      clusterOptions = markerClusterOptions()
    ) %>%
  addLegend(
    position = 'bottomright', 
    colors= c("green", "orange", "red"), labels=c("Entire Home/Apt", "Private Room", "Shared Room"), 
    title='Types of Rentals', 
  )
```



### Histograms
```{r, warning=FALSE}
library(ggplot2)
library(ggthemes)

airbnb_home <- airbnb_tab %>%
  filter(room_type == 'Entire home/apt')

knitr::kable(head(airbnb_home, n=10))
```


```{r}
airbnb_home %>%
  ggplot(aes(x = neighbourhood_group, y = price)) +
  geom_boxplot()+
  coord_flip() +
  theme_economist() + 
  scale_fill_economist() +
  labs(title = "Entire Homes & Appts. Price By Neighborhood in 2019",
       x = "Major Neighborhood Groups",
       y = "Price(USD)")
```


```{r, warning = FALSE}
airbnb_home %>%
  ggplot(aes(x = neighbourhood_group, y = price)) +
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 1500)) +
  coord_flip() +
  theme_economist() + 
  scale_fill_economist() +
  labs(title = "2019 NYC Homes & Appts. Prices (Up to $1500/night)",
       x = "Major Neighborhood Groups",
       y = "Price(USD)")
```



```{r}
airbnb_room <- airbnb_tab %>%
  filter(room_type == 'Private room')

knitr::kable(head(airbnb_room, n=10))
```
```{r}
airbnb_room %>%
  ggplot(aes(x = neighbourhood_group, y = price)) +
  geom_boxplot()+
  coord_flip() +
  theme_economist() + 
  scale_fill_economist() +
  labs(title = "Private Room Price By Neighborhood in 2019",
       x = "Major Neighborhood Groups",
       y = "Price(USD)")
```

```{r, warning = FALSE}
airbnb_room %>%
  ggplot(aes(x = neighbourhood_group, y = price)) +
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 500)) +
  coord_flip() +
  theme_economist() + 
  scale_fill_economist() +
  labs(title = "2019 NYC Private Room Prices (Up to $500/night)",
       x = "Major Neighborhood Groups",
       y = "Price(USD)")
```




```{r}
airbnb_sroom <- airbnb_tab %>%
  filter(room_type == 'Shared room')

knitr::kable(head(airbnb_sroom, n=10))
```


```{r}
airbnb_sroom %>%
  ggplot(aes(x = neighbourhood_group, y = price)) +
  geom_boxplot()+
  coord_flip() +
  theme_economist() + 
  scale_fill_economist() +
  labs(title = "Shared Room Price By Neighborhood in 2019",
       x = "Major Neighborhood Groups",
       y = "Price(USD)")
```

```{r, warning = FALSE}
airbnb_sroom %>%
  ggplot(aes(x = neighbourhood_group, y = price)) +
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 200)) +
  coord_flip() +
  theme_economist() + 
  scale_fill_economist() +
  labs(title = "2019 NYC Shared Room Prices (Up to $200/night)",
       x = "Major Neighborhood Groups",
       y = "Price(USD)")
```


# Hypothesis Testing & Machine Learning

## Linear Regression

With datasets that are large, it can be very useful to generate a linear regression, or a line of "best fit", for an easier interpretation of the data. This data analysis technique is also an effective way to learn about general trends of our data set and lets us construct confidence intervals and do hypothesis testing, which analyzes and tests for relationships between variables.

We want to look at the relationship between price and distance away from Times Square in New York City, one of the largest populated cities in New York. We are looking at Time Square since it is a major commercial intersection, tourist destination, entertainment center, and neighborhood in the Midtown Manhattan section of NYC ([Wikipedia](https://en.wikipedia.org/wiki/Times_Square)).

For these reasons, we would like to see if Airbnb listings would increase as their distance to Times Square (latitude 40.757, longitude -73.986) decreases, and vice versa. We will be using functions from the `geosphere` library to calculate distance between coordinates.


**First**, let's add an attribute called `distToTimesSquare` in our dataset. This will contain the distance (in miles) between each listing and Times Square.
```{r}

coordsTimeSquare <- c(-73.986, 40.757)

airbnb_tab <- airbnb_tab %>%
  mutate(distToTimesSquare = by(airbnb_tab, 1:nrow(airbnb_tab), 
                                function(row) { 
                                  distHaversine(c(row$longitude, row$latitude), coordsTimeSquare)
                                }) / 1609) # divide by 1609 to convert meters to miles

knitr::kable(head(airbnb_tab))
```


**Second**, let's split our current `airbnb_tab` data frame into two data frames, one with `room_type == "Entire home/apt"` and one with `room_type == "Private room"`. This is because prices are much more expensive for "Entire home/apt" listings, so we don't want to get confused when regressing against distance. We only want to see the relation between distance and prices, not between prices and size of the space being listed!

```{r}
# create new dataframe of listings where room_type=="Entire home/apt"
entire_tab <- airbnb_tab %>%
  filter(room_type == "Entire home/apt")

# create new dataframe of listings where room_type=="Private room"
private_tab <- airbnb_tab %>%
  filter(room_type == "Private room")

shared_tab <- airbnb_tab %>%
  filter(room_type == "Shared room")

knitr::kable(head(entire_tab))
knitr::kable(head(private_tab))
knitr::kable(head(shared_tab))
```


**Third**, we want to create a scatter plot of the prices of listings against their distance to Times Square. We'll also add a regression line to this scatter plot to the general increasing or decreasing trend in our data! Let's do this three times, once for each room_type we are interested in.

```{r, warning=FALSE}
entire_tab %>%
    ggplot(aes(x=entire_tab$distToTimesSquare,y=entire_tab$price)) +
    geom_point() + # plot points for scatter plot
    geom_smooth(method=lm) + # plot linear regression line or line of best fit
    ylim(0, 1500) + # set the upper limit of prices to $1500
    labs(title="Homes & Appts. Prices vs Distance to Times Square", x="Distance to Times Square (miles)", y="Price (USD)")

private_tab %>%
    ggplot(aes(x=private_tab$distToTimesSquare,y=private_tab$price)) +
    geom_point() + # plot points for scatter plot
    geom_smooth(method=lm) + # plot linear regression line or line of best fit
    ylim(0, 500) + # set the upper limit of prices to $500
    labs(title="Private Room Prices vs Distance to Times Square", x="Distance to Times Square (miles)", y="Price (USD)")

shared_tab %>%
    ggplot(aes(x=shared_tab$distToTimesSquare,y=shared_tab$price)) +
    geom_point() + # plot points for scatter plot
    geom_smooth(method=lm) + # plot linear regression line or line of best fit
    ylim(0, 200) + # set the upper limit of prices to $200
    labs(title="Shared Room Prices vs Distance to Times Square", x="Distance to Times Square (miles)", y="Price (USD)")
```

**Lastly**, let's analyze the resulting models quantitatively using `broom::tidy`.

```{r}
entire_fit <- lm(distToTimesSquare~price, data=entire_tab)
entire_fit %>%
  tidy()

private_fit <- lm(distToTimesSquare~price, data=private_tab)
private_fit %>%
  tidy()

shared_fit <- lm(distToTimesSquare~price, data=shared_tab)
shared_fit %>%
  tidy()
```

As we can see in all three of these linear regression plots, **the prices of all the types of listing decreases slowly as the location of the listing gets further away from Times Square**. From the models, it is clear that prices of Airbnb listings decrease by 0.00149 (homes and apts), 0.00237 (private rooms), and 0.00555 (shared rooms) on average each mile further away from Times Square.


Even though we can clearly see a trend in our linear regressions, it is best to conduct **hypothesis testing** in order to determine if our results are valid and there is a significantly meaningful relationship between Airbnb prices and their distance away from high traffic locations, such as Times Square in New York City ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/)).

Let's ask the question: *Do we reject the null hypothesis of no relationship between price and distance from Times Square?*

Our answer: Yes, we reject the null hypothesis since the p-values for all three linear regressions are significantly smaller than 0.05. A p-value less than or equal to 0.05 means that the results for our data holds, that our data is repeatable, and that our results didn't just happen by chance ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/)).


You can read more about Linear Regression at [CMSC 320 Linear Regression Lecture Notes](https://www.hcbravo.org/IntroDataSci/bookdown-notes/linear-regression.html#simple-regression) by Professor Hector Corrada Bravo.



## ML Model




# Conclusion


