---
title: "3 Steps To Become A Data Science Pro"
author: "Priyanka Kishore & Alisha Varma"
date: "5/17/2020"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
So you want to be a Data Scientist and don’t know where to start? Well, you’ve come to the right place. 

Today you’ll learn a little about the economy, a whole bunch of data science principles and practice and where to stay on your next trip to New York City!


## Data Science Pipeline
First, let’s look at the data science pipeline. 

The data science pipeline starts with defining what major questions one wants to answer and subsequently **_acquiring and importing the relevant data_** to be analyzed.

Then, the data is viewed and **_data tidying_** must occur; where a rectangular data structure model is assumed and three requirements must be met. Each observation (called an entity) forms a row, each variable (called an attribute) forms a column and each observational unit (type of entity) forms a table.

Leading to the exploratory data analysis process, where the data is transformed and visualized. **_Data cleaning_** may be necessary for missing data. When handling missing data, the missing data may be removed, encoded or imputation (replace missing values with the mean of non-missing values) of a numeric variable may be necessary.

Hypothesis testing and machine learning (ML) modeling are the final steps before the data and its results can be communicated.

![Image of Pipeline](https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png)

_Source:_ https://r4ds.had.co.nz/explore-intro.html

## Economy & Vacations
The rise of companies like Airbnb has given rise to The Sharing Economy. The sharing economy is a model defined as the facilitation of goods and services on a peer-to-peer level usually through online community platforms. This new model has made it possible for a great deal of people to gain another source of income and for you to have an affordable vacation.

As more sharing economy companies have opened, like Airbinb and Uber, the way we vacation has changed. This change has been documented and open data on it is available.


## DataSet Used
The data we will be using in this tutorial is [_New York City Airbnb Open Data_](http://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data/data) from Kaggle. We will use this data to look at the relationships between types of housing and location.


# Preparing Data

Download the [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv).

In this section, we will learn how to load in our dataset, view the data in our dataset, and clean it up so it's easy for us to work with.

First, let's load in the following libraries so we can use certain functions:
```{r, message=FALSE}
# for data wranging
library(tidyverse)
library(dplyr)
```

## Loading Data

CSV files are files that include data which are "comma-separated values", meaning that data values are literally separated by commas.

After we've downloaded our CSV file from Kaggle into our working directory, we can use the `read_csv` function to load the CSV file data into our program's data frame, which is a table of the data.

```{r, message=FALSE}
# create a dataframe from our CSV file
airbnb_tab <- read.csv("AB_NYC_2019.csv", header=TRUE)
```

There are some attributes that we don't need for our purposes, like `host_id`, `host_name`, `minimum_nights`, `number_of_reviews`, `last_review`, `reviews_per_month`, and `calculated_host_listings_count`. So, let's remove these from our data frame:

```{r}
# a vector called "to_remove" that has the names of the attributes we don't want
to_remove <- c('host_id', 'host_name', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count')

# removing attributes from data frame using "to_remove"
airbnb_tab = airbnb_tab[ , !(names(airbnb_tab) %in% to_remove)]
```

## Viewing Data

Here, we see the first 10 rows in our dataset:

```{r}
head(airbnb_tab, n=10)
```

The following is a list of descriptions for the attributes of our data set:

Attribute | Description/Unit
------------ | -------------
`id` | Unique ID for each Airbnb listing
`name` | Name or description of the Airbnb listing
`neighbourhood_group` | Boroughs of New York (Manhattan, Brooklyn, Queens, Bronx, Staten Island)
`neighbourhood` | Neighborhoods of New York
`latitude` | Degrees of latitude, measures distance North and South from Equator
`longitude` | Degrees of longitude, measure distance East and West of Prime Meridian
`room_type` | Type of space offered (Entire home/apt, Private room, Shared room)
`price` | Price of listing, in US Dollars
`availability_365` | Number of days in a year when the listing is available for booking


## Tidying Data

Tidying Data entails the elements listed in the list below.

Elements of a tidy dataset:
1. Each observation/entity forms a row
1. Each variable/attribute forms a column
1. Each observational unit (type of entity) forms a column (i.e. not dependent on one another)

Our dataset is already tidy and meets the criteria above. Each entity is a row and each attribute is a column, where no entity is dependent on another.

However, if your data set is untidy, below is an example on a different small dataset, to show you what to do.

### Sample Tidying



# Exploratory Data Analysis

## Handling Missing Data

Recall that the attribute `availability_365` tells us how many days in the year that this particular listing is available for people to book. 

Notice that 0 is a value for some of the entities (Airbnb listings). It doesn't make much sense for us to look at entities that aren't available at all during the year. In fact, more than 17000 entities are listed at being available for 0 days out of the year! That's about 1/3 of our dataset.

We'll call this "missing data", and remove these entities from our dataset:

```{r}

airbnb_tab <- airbnb_tab %>%
  filter(availability_365 > 0)

head(airbnb_tab, n=10)

```



## Visualizations




# Hypothesis Testing & Machine Learning

## Linear Regression


## ML Model




# Conclusion



# References
A thorough R-markdown guide https://bookdown.org/yihui/rmarkdown/
